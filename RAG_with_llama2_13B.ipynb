{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG with Llama 2 and LangChain\n",
        "Retrieval-Augmented Generation (RAG) is a methodology that integrates a retriever and a generative language model to provide precise responses. This approach entails retrieving pertinent details from an extensive corpus and subsequently generating responses that align contextually with queries. In this instance, we apply the quantized iteration of the Llama 2 13B Language Model (LLM) in conjunction with LangChain for generative Question-Answering (QA) specifically on a document reporting Ericsson's Q4 2023 earnings. The notebook file has been validated on Google Colab using a T4 GPU. Please ensure that the runtime type is set to T4 GPU before executing the notebook."
      ],
      "metadata": {
        "id": "ce4-y4JJgVSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Packages"
      ],
      "metadata": {
        "id": "tPpiHVgj4CmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers>=4.32.0 optimum>=1.12.0\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
        "!pip install langchain\n",
        "!pip install chromadb\n",
        "!pip install sentence_transformers # ==2.2.2\n",
        "!pip install unstructured\n",
        "!pip install pdf2image\n",
        "!pip install pdfminer.six\n",
        "!pip install unstructured-pytesseract\n",
        "!pip install unstructured-inference\n",
        "!pip install faiss-gpu\n",
        "!pip install pikepdf\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "zjLkm65J_S0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart Runtime"
      ],
      "metadata": {
        "id": "30AIKqF9rTuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Llama 2\n",
        "We will use the quantized version of the LLAMA 2 13B model from HuggingFace for our RAG task. It runs faster."
      ],
      "metadata": {
        "id": "X22Ccat1oXE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "model_name = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001 # 0.0\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "N_hQ_IfuoWT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test LLM with Llama 2 prompt structure and LangChain PromptTemplate"
      ],
      "metadata": {
        "id": "R69LfWsnYAqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "You are an AI assistant. You are truthful, unbiased and honest in your response.\n",
        "\n",
        "If you are unsure about an answer, truthfully say \"I don't know\"\n",
        "<</SYS>>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "text = \"Explain artificial intelligence in a few lines\"\n",
        "result = llm(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ],
      "metadata": {
        "id": "BX6bnnL45a2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "M-BEF0Zgbsrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG from web pages\n",
        "### A. Create a vectore store for the context/external data\n",
        "In this process, we will generate embedding vectors for the unstructured data obtained from the source and then store them in a vector store."
      ],
      "metadata": {
        "id": "a2SNJZEtlDo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Load the document\n",
        "\n",
        "Depending on the type of the source data, we can use the appropriate data loader from LangChain to load the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "ffMdNNqecUq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "\n",
        "web_loader = UnstructuredURLLoader(\n",
        "    urls=[\"https://mb.cision.com/Main/15448/3913672/2555477.pdf\"], mode=\"elements\", strategy=\"fast\",\n",
        "    )\n",
        "web_doc = web_loader.load()\n",
        "updated_web_doc = filter_complex_metadata(web_doc)"
      ],
      "metadata": {
        "id": "z8Y3YnRjJGEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Split the documents into chunks\n",
        "\n",
        "Due to the limited size of the context window of an LLM, the data need to be divided into smaller chunks with a text splitter like ``CharacterTextSplitter`` or ``RecursiveCharacterTextSplitter``. In this way, the smaller chunks can be fed into the LLM."
      ],
      "metadata": {
        "id": "5rS8wix4cnKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
        "chunked_web_doc = text_splitter.split_documents(updated_web_doc)\n",
        "len(chunked_web_doc)"
      ],
      "metadata": {
        "id": "GVjSIuPMmYvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a vector database of the chunked documents with HuggingFace embeddings"
      ],
      "metadata": {
        "id": "k03r4pKIcyAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "R3hqEWvUcyBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can either use Chroma or FAISS to create the [Vector Store](https://python.langchain.com/docs/modules/data_connection/vectorstores.html)."
      ],
      "metadata": {
        "id": "u86xXj9v8cPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Create the vectorized db with FAISS\n",
        "from langchain.vectorstores import FAISS\n",
        "db_web = FAISS.from_documents(chunked_web_doc, embeddings)\n",
        "\n",
        "# Create the vectorized db with Chroma\n",
        "# from langchain.vectorstores import Chroma\n",
        "# db_web = Chroma.from_documents(chunked_web_doc, embeddings)"
      ],
      "metadata": {
        "id": "gA3xtLSQmh-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Use RetrievalQA chain\n",
        "We initialize a RetrievalQA chain using LangChain, incorporating a retriever, LLM, and a specified chain type as input parameters. When the QA chain is presented with a query, the retriever retrieves pertinent information from the vector store. The method \"chain type = \"stuff\"\" consolidates all retrieved information into context and triggers a call to the language model. Subsequently, the LLM generates the text or response based on the retrieved documents. Additional details on LangChain Retriever can be found [here](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa).\n",
        "\n",
        "**LLM Prompt Structure**\n",
        "\n",
        "It is also possible to provide the recommended prompt structure for Llama 2 for QA purposes. This approach allows us to guide the LLM to solely utilize the available context to answer questions. If the context lacks information relevant to the query, the LLM will refrain from fabricating an answer and instead indicate its inability to find pertinent information in the given context."
      ],
      "metadata": {
        "id": "agfSAbN_zvYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# use the recommended propt style for the LLAMA 2 LLM\n",
        "prompt_template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
        "\n",
        "<</SYS>>\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "Chain_web = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    # retriever=db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 5, 'score_threshold': 0.8})\n",
        "    # Similarity Search is the default way to retrieve documents relevant to a query, but we can use MMR by setting search_type = \"mmr\"\n",
        "    # k defines how many documents are returned; defaults to 4.\n",
        "    # score_threshold allows to set a minimum relevance for documents returned by the retriever, if we are using the \"similarity_score_threshold\" search type.\n",
        "    # return_source_documents=True, # Optional parameter, returns the source documents used to answer the question\n",
        "    retriever=db_web.as_retriever(), # (search_kwargs={'k': 5, 'score_threshold': 0.8}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "query = \"What are our top 5 countries in sale this quarter?\"\n",
        "result = Chain_web.invoke(query)\n",
        "result"
      ],
      "metadata": {
        "id": "GxfNkw_PnItp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "id": "DezXfGwjCQv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "query = \"What are the top 5 countries in sale 2023 quarter percentages\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "id": "jfraMfszR6BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Hallucination Check\n",
        "Hallucination in RAG refers to the generation of content by an LLM that is not based onn the retrieved knowledge.\n",
        "\n",
        "Let's test our LLM with a query that is not relevant to the context. The model should respond that it does not have enough information to respond to this query."
      ],
      "metadata": {
        "id": "L5l8ucgMjuLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "query = \"How does the tranformers architecture work?\"\n",
        "result = Chain_web.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "id": "EWgQgd8FJcBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model responded as expected. The context provided to it do not contain any information on tranformers architectures. So, it cannot answer this question!"
      ],
      "metadata": {
        "id": "-rwp6YEAkApG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG from PDF Files"
      ],
      "metadata": {
        "id": "JgSv0RrUPaDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download pdf files"
      ],
      "metadata": {
        "id": "JMoYqFnc4XMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load PDF Files"
      ],
      "metadata": {
        "id": "Bzm9gKiFPoD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "pdf_loader = UnstructuredPDFLoader(\"/content/Earnings.pdf\")\n",
        "pdf_doc = pdf_loader.load()\n",
        "updated_pdf_doc = filter_complex_metadata(pdf_doc)"
      ],
      "metadata": {
        "id": "2O0JxHkUQsHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spit the document into chunks"
      ],
      "metadata": {
        "id": "QEfY__rf4BjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
        "chunked_pdf_doc = text_splitter.split_documents(updated_pdf_doc)\n",
        "len(chunked_pdf_doc)"
      ],
      "metadata": {
        "id": "--JjeRqGTwH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the vector store"
      ],
      "metadata": {
        "id": "s4s2c_Q547N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)"
      ],
      "metadata": {
        "id": "fXDUlIAsUsRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG with RetrievalQA"
      ],
      "metadata": {
        "id": "I4Jx0m0e5J9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "Chain_pdf = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db_pdf.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "query = \"What are our top 5 countries in sale this quarter?\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "id": "9-InFSJZVtTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hallucination Check"
      ],
      "metadata": {
        "id": "4LWO2cgzgHS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "query = \"How does the tranformers architecture work?\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ],
      "metadata": {
        "id": "FgUFV7nti60t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}